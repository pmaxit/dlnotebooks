{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mllib.nlp.datasets.cmudict import CMUDict\n",
    "from datasets import load_dataset\n",
    "from mllib.nlp.seq2seq import Seq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset cmu_dict (/root/.cache/huggingface/datasets/cmu_dict/cmu3/1.0.0/a0e598136ef9603a0d6d97059f5e1d2cac789cfe3c0998cb2b4b7fd4198da504)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a60471dc210a4c4aa108c57ebf36ed2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds = load_dataset('/notebooks/dlnotebooks/mllib/nlp/datasets/cmudict.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached split indices for dataset at /root/.cache/huggingface/datasets/cmu_dict/cmu3/1.0.0/a0e598136ef9603a0d6d97059f5e1d2cac789cfe3c0998cb2b4b7fd4198da504/cache-b2d95f15faaf4500.arrow and /root/.cache/huggingface/datasets/cmu_dict/cmu3/1.0.0/a0e598136ef9603a0d6d97059f5e1d2cac789cfe3c0998cb2b4b7fd4198da504/cache-5ed2c4bc219f1934.arrow\n"
     ]
    }
   ],
   "source": [
    "train_test = ds['train'].train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'word': 'preempted',\n",
       " 'word_length': 9,\n",
       " 'phoneme': ['P', 'R', 'IY0', 'EH1', 'M', 'P', 'T', 'IH0', 'D']}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processing data with map inspired by `tf.dataset.map` map method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phoneme_vocab = build_vocab_from_iterator(train_test['train']['phoneme'],specials=['<unk>','<sos>'])\n",
    "word_vocab = build_vocab_from_iterator(train_test['train']['word'],specials=['<unk>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phoneme_vocab(['<unk>','<sos>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phoneme_vocab.set_default_index(0)\n",
    "word_vocab.set_default_index(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 17, 10]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vocab.lookup_indices(['a','b','c'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_pipeline = lambda data: word_vocab.lookup_indices(list(data))\n",
    "trg_pipeline = lambda data: phoneme_vocab.lookup_indices(data)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare for manual inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# from python dictionary\n",
    "my_dict = {\n",
    "    'word': ['puneet','mike'],\n",
    "    'word_length': [6,4],\n",
    "    'phoneme': [['P','N','T'],['M','K']]\n",
    "}\n",
    "\n",
    "dummy_dataset= Dataset.from_dict(my_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "def pad_collate(batch):\n",
    "    items= [ (src_pipeline(items['word']), \n",
    "             trg_pipeline(items['phoneme']), \n",
    "              items['word_length']) for items in batch\n",
    "           ]\n",
    "    \n",
    "    def pad(xs, ind):\n",
    "        xs = [torch.tensor(x[ind]) for x in xs]\n",
    "        return torch.nn.utils.rnn.pad_sequence(xs, batch_first=True)\n",
    "\n",
    "    return {\n",
    "        'src': pad(items, 0),\n",
    "        'trg' : pad(items, 1),\n",
    "        'src_len' : torch.tensor([x[2] for x in items])\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = DataLoader(train_test['train'], shuffle=True, collate_fn=pad_collate, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding, default_data_collator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "src irell<unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>\n",
      "src catanzaro<unk><unk><unk><unk><unk><unk>\n",
      "src nayden<unk><unk><unk><unk><unk><unk><unk><unk><unk>\n",
      "src lucie<unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>\n",
      "src moore<unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>\n",
      "------------------------------------------------------------------------\n",
      "trg AY0,R,EH1,L,<unk>,<unk>,<unk>,<unk>,<unk>,<unk>,<unk>,<unk>,<unk>,<unk>\n",
      "trg K,AA0,T,AA0,N,Z,AA1,R,OW0,<unk>,<unk>,<unk>,<unk>,<unk>\n",
      "trg N,EY1,D,IH0,N,<unk>,<unk>,<unk>,<unk>,<unk>,<unk>,<unk>,<unk>,<unk>\n",
      "trg L,UW1,S,IY0,<unk>,<unk>,<unk>,<unk>,<unk>,<unk>,<unk>,<unk>,<unk>,<unk>\n",
      "trg M,UH1,R,<unk>,<unk>,<unk>,<unk>,<unk>,<unk>,<unk>,<unk>,<unk>,<unk>,<unk>\n",
      "------------------------------------------------------------------------\n",
      "src_len tensor(5)\n",
      "src_len tensor(9)\n",
      "src_len tensor(6)\n",
      "src_len tensor(5)\n",
      "src_len tensor(5)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "# checking\n",
    "def decode_word(lst):\n",
    "    return ''.join(word_vocab.lookup_tokens(lst.numpy()))\n",
    "\n",
    "def decode_phoneme(lst):\n",
    "    return ','.join(phoneme_vocab.lookup_tokens(lst.numpy()))\n",
    "\n",
    "decoder_funs={\n",
    "    'src': decode_word,\n",
    "    'trg': decode_phoneme\n",
    "}\n",
    "\n",
    "batch=next(iter(dls))\n",
    "\n",
    "\n",
    "for k,v in batch.items():\n",
    "    print('---'*24)\n",
    "    count = 5\n",
    "    for i in range(len(v)):\n",
    "        if count == 0:\n",
    "            continue\n",
    "        if k in decoder_funs:\n",
    "            print(k, decoder_funs[k](v[i]))\n",
    "        else:\n",
    "            print(k,v[i])\n",
    "            \n",
    "        count -= 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NeptuneLogger will work in online mode\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers import NeptuneLogger\n",
    "\n",
    "# create NeptuneLogger\n",
    "neptune_logger = NeptuneLogger(\n",
    "    api_key = \"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiIwYWY0OTQ4MS03MGY4LTRhNjUtOTFlZC0zZjVjMjlmZGQxNjQifQ==\",\n",
    "\n",
    "    project_name='puneetgirdhar.in/seq2seq',  # \"<WORKSPACE/PROJECT>\"\n",
    "    tags=[\"seq2seq\"],  # optional\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MAX_EPOCHS=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def evalute(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
    "#     with torch.no_grad():\n",
    "#         input_tensor = tensorFromSentence(sentence)\n",
    "#         input_length = input_tensor.size()[0]\n",
    "        \n",
    "#         decoder_input = torch.ones().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvaluationCallback(Callback):\n",
    "    def __init__(self, num_samples:int = 3, max_length:int=10, pad_idx:int = 0):\n",
    "        self.num_samples = num_samples\n",
    "        \n",
    "        self.pad_idx = pad_idx\n",
    "        self.examples  = {\n",
    "        'word': ['puneet','mike'],\n",
    "        'word_length': [6,4],\n",
    "        'phoneme': [['P','N','T','P','T','S'],['M','K']]\n",
    "        }\n",
    "        \n",
    "        \n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def setup(self,trainer, model, stage):\n",
    "        dummy_dataset= Dataset.from_dict(self.examples)\n",
    "        \n",
    "        self.dls = DataLoader(dummy_dataset, shuffle=True, collate_fn = pad_collate, batch_size=4)\n",
    "        self.batch = next(iter(self.dls))\n",
    "        \n",
    "    def on_epoch_end(self,trainer,pl_module)->None:\n",
    "\n",
    "        input_seq = self.batch['src'].cuda()\n",
    "        target_seq =  self.batch['trg'].cuda()\n",
    "        src_len = self.batch['src_len'].cuda()\n",
    "        output = pl_module(input_seq, src_len, target_seq)\n",
    "        \n",
    "        output = output.permute(1, 0,2) # batch x seq_len x output_dim\n",
    "        \n",
    "        topv, topi = output.topk(1,dim=-1)\n",
    "        # topi will be seq_lenx batch x output_dim\n",
    "        \n",
    "        topi= topi.squeeze(dim=-1)\n",
    "        for b in range(topi.shape[0]):\n",
    "            result = decode_phoneme(topi[b].cpu())\n",
    "            print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: None, using: 0 TPU cores\n",
      "GPU available: True, used: False\n",
      "TPU available: None, using: 0 TPU cores\n"
     ]
    }
   ],
   "source": [
    "lr_monitor = LearningRateMonitor(logging_interval='step')\n",
    "ec = EvaluationCallback()\n",
    "logger = TensorBoardLogger('tb_logs', name='my_model')\n",
    "trainer = pl.Trainer(callbacks=[lr_monitor, ec],max_epochs=MAX_EPOCHS, gpus=1, logger=[neptune_logger])\n",
    "\n",
    "find_trainer = pl.Trainer(auto_lr_find=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://app.neptune.ai/puneetgirdhar.in/seq2seq/e/SEQ-4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name    | Type             | Params\n",
      "---------------------------------------------\n",
      "0 | _loss   | CrossEntropyLoss | 0     \n",
      "1 | encoder | Encoder          | 14.0 K\n",
      "2 | decoder | NewDecoder       | 17.3 K\n",
      "---------------------------------------------\n",
      "31.3 K    Trainable params\n",
      "0         Non-trainable params\n",
      "31.3 K    Total params\n",
      "0.125     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:51: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2edbfcf432f34ef687d2dd5f02541167",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.fit(model, train_dataloader=dls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_vocab_size = len(word_vocab)\n",
    "output_vocab_size  = len(phoneme_vocab)\n",
    "\n",
    "model = Seq2Seq(input_vocab_size, output_vocab_size,p=0.1,max_epochs=MAX_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:51: UserWarning: you defined a validation_step but have no val_dataloader. Skipping validation loop\n",
      "  warnings.warn(*args, **kwargs)\n",
      "\n",
      "  | Name    | Type             | Params\n",
      "---------------------------------------------\n",
      "0 | _loss   | CrossEntropyLoss | 0     \n",
      "1 | encoder | Encoder          | 14.0 K\n",
      "2 | decoder | NewDecoder       | 17.3 K\n",
      "---------------------------------------------\n",
      "31.3 K    Trainable params\n",
      "0         Non-trainable params\n",
      "31.3 K    Total params\n",
      "0.125     Total estimated model params size (MB)\n",
      "/opt/conda/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:51: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bbd7e6dcf2c4e169d1caea6063f8835",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restored states from the checkpoint file at /notebooks/dlnotebooks/nbs/lr_find_temp_model.ckpt\n"
     ]
    }
   ],
   "source": [
    "lr_finder = find_trainer.tuner.lr_find( model, train_dataloader=dls )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#lr_finder.results\n",
    "# Plot with\n",
    "fig = lr_finder.plot(suggest=True)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick point based on plot, or get suggestion\n",
    "new_lr = lr_finder.suggestion()\n",
    "\n",
    "# update hparams of the model\n",
    "model.hparams.lr = new_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/pytorch_lightning/utilities/distributed.py:51: UserWarning: you defined a validation_step but have no val_dataloader. Skipping validation loop\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "setup() takes 1 positional argument but 4 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-9697e6b32906>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, train_dataloader, val_dataloaders, datamodule)\u001b[0m\n\u001b[1;32m    455\u001b[0m         \u001b[0;31m# SET UP TRAINING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m         \u001b[0;31m# ----------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_setup_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"on_before_accelerator_backend_setup\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# note: this sets up self.lightning_module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mcall_setup_hook\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m   1061\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcalled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1062\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatamodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstage_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1063\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstage_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1064\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstage_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1065\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/callback_hook.py\u001b[0m in \u001b[0;36msetup\u001b[0;34m(self, model, stage)\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;34m\"\"\"Called in the beginning of fit and test\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mteardown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: setup() takes 1 positional argument but 4 were given"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainer.save_checkpoint('model.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will test the model to check if it has been trained well or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Seq2Seq.load_from_checkpoint('model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0005\n"
     ]
    }
   ],
   "source": [
    "print(model.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
