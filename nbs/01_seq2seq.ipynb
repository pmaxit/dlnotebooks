{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "environmental-location",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp nlp.seq2seq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "durable-liberal",
   "metadata": {},
   "source": [
    "# Encoder - Decoder Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "utility-worker",
   "metadata": {},
   "source": [
    "A Recurrent neural network, or RNN is a network that operates on a sequence and uses its own output as input for subsequent steps.\n",
    "\n",
    "A `Sequence to Sequence` network or seq2seq network i a model consisting of two RNNs called the encoder and decoder. The encoder reads an input sequence and outputs a single vector, and decoder reads the vector to produce an output sequence.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lesbian-rates",
   "metadata": {},
   "source": [
    "![](https://pytorch.org/tutorials/_images/seq2seq.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latest-composition",
   "metadata": {},
   "source": [
    "Unlike sequence prediction with a single RNN, where every input corresponds to an output,the seq2seq model frees us from sequence length and order, which makes it idea for translation between two languages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "widespread-folks",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contained-tuesday",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "applicable-magnet",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, num_layers=2, p=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.dropout = nn.Dropout(p)\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=p,batch_first=False)\n",
    "\n",
    "    def forward(self, x, x_len):\n",
    "        # x shape (seq_length, N)\n",
    "        embedding = self.dropout(self.embedding(x))\n",
    "        # embedding shape : (seq_length, N, embedding_size)\n",
    "        \n",
    "        x_packed = pack_padded_sequence(embedding, x_len.cpu(), batch_first=False, enforce_sorted=False)\n",
    "        output_packed, (hidden,cell) = self.rnn(x_packed)\n",
    "        \n",
    "        # irrelevant because we are interested only in hidden state\n",
    "        #output_padded, output_lengths = pad_packed_sequence(output_packed, batch_first=True)\n",
    "        \n",
    "        # output is irrelevant, context vector is important\n",
    "        return hidden,cell\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjacent-motivation",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class NewDecoder(nn.Module):\n",
    "    def __init__(self, hidden_size, embedding_size, output_size, n_layers=1, dropout_p=0.1):\n",
    "        super(NewDecoder, self).__init__()\n",
    "        \n",
    "        # Define parameters\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers =n_layers\n",
    "        self.dropout_p = dropout_p\n",
    "        \n",
    "        # Define layers\n",
    "        self.embedding = nn.Embedding(output_size, embedding_size)\n",
    "        self.dropout=nn.Dropout(dropout_p)\n",
    "        self.rnn = nn.LSTM(embedding_size, hidden_size, n_layers, dropout=dropout_p, batch_first=False)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, word_input, last_hidden, encoder_outputs):\n",
    "        # Note that we will only be running forward for a single decoder time step, but will\n",
    "        # use all encoder outputs\n",
    "        word_input = word_input.unsqueeze(0)\n",
    "        # we are not using encoder_outputs here\n",
    "        word_embedded = self.embedding(word_input) # 1 X B\n",
    "        word_embedded = self.dropout(word_embedded)  # 1 X B X emb_length\n",
    "        \n",
    "        # Combine embedded input word and hidden vector, run through RNN\n",
    "        output, hidden = self.rnn(word_embedded, last_hidden)  # 1 X B X hidden\n",
    "        predictions = self.out(output)     # 1, B, out\n",
    "        #output = F.log_softmax(predictions)\n",
    "        \n",
    "        return predictions, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "painted-noise",
   "metadata": {},
   "source": [
    "# Trainer Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "limited-profession",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import random\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import pytorch_lightning.metrics.functional as plfunc\n",
    "from pytorch_lightning.loggers import TensorBoardLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "applicable-treasury",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Seq2Seq(pl.LightningModule):\n",
    "    \"\"\" Encoder decoder pytorch lightning module for training seq2seq model with teacher forcing\n",
    "    Module try to learn mapping from one sequence to another\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def add_model_specific_args(parent_parser):\n",
    "        parser = ArgumentParser(parents=[parent_parser], add_help=False)\n",
    "        parser.add_argument(\"--emb_dim\", type=int, default=32)\n",
    "        parser.add_argument('--hidden_dim', type=int, default=64)\n",
    "        parser.add_argument('--dropout', type=float, default=0.1)\n",
    "        \n",
    "        return parser\n",
    "    \n",
    "    def __init__(self,\n",
    "                input_vocab_size,\n",
    "                output_vocab_size,\n",
    "                padding_index = 0,\n",
    "                emb_dim = 8,\n",
    "                hidden_dim=32,\n",
    "                dropout=0.1,\n",
    "                max_length=20,\n",
    "                **kwargs):\n",
    "        super().__init__()\n",
    "        \n",
    "        \n",
    "        # dynamic, based on tokenizer vocab size defined in datamodule\n",
    "        self.input_dim = input_vocab_size\n",
    "        self.output_dim = output_vocab_size\n",
    "        \n",
    "        self.enc_emb_dim = emb_dim\n",
    "        self.dec_emb_dim = emb_dim\n",
    "        \n",
    "        self.enc_hid_dim = hidden_dim\n",
    "        self.dec_hid_dim = hidden_dim\n",
    "        \n",
    "        self.enc_dropout = dropout\n",
    "        self.dec_dropout = dropout\n",
    "        \n",
    "        self.pad_idx = padding_index\n",
    "        self.num_layers = 2\n",
    "        self.max_length =10\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.max_epochs= kwargs.get('max_epochs',5)\n",
    "        \n",
    "        self.learning_rate = 0.0005\n",
    "        \n",
    "        self._loss = nn.CrossEntropyLoss(ignore_index=self.pad_idx)\n",
    "        \n",
    "        self.encoder = Encoder(\n",
    "            self.input_dim,\n",
    "            self.enc_emb_dim,\n",
    "            self.enc_hid_dim,\n",
    "            self.num_layers,\n",
    "            self.enc_dropout\n",
    "        )\n",
    "        \n",
    "        self.decoder = NewDecoder(\n",
    "            self.enc_hid_dim,\n",
    "            self.dec_emb_dim,\n",
    "            self.output_dim,\n",
    "            self.num_layers,\n",
    "            self.dec_dropout\n",
    "        )\n",
    "        \n",
    "        self._init_weights()\n",
    "        \n",
    "    def _init_weights(self):\n",
    "        for name, param in self.named_parameters():\n",
    "            if \"weight\" in name:\n",
    "                nn.init.normal_(param.data, mean=0, std=0.01)\n",
    "            \n",
    "            else:\n",
    "                nn.init.constant_(param.data, 0)\n",
    "    \n",
    "    def create_mask(self, src):\n",
    "        mask = (src != self.pad_idx).permute(1, 0)\n",
    "        return mask\n",
    "    \n",
    "    \n",
    "    \n",
    "    def forward(self, src_seq, source_len, trg_seq, teacher_force_ratio=0.5):\n",
    "        \"\"\"\n",
    "        teacher_force_ratio is used to help in decoding.\n",
    "        In starting, original input token will be sent as input token\n",
    "        \"\"\"\n",
    "        source = src_seq.transpose(0, 1)\n",
    "        target_len = self.max_length\n",
    "        \n",
    "        if trg_seq is not None:\n",
    "            target = trg_seq.transpose(0, 1)\n",
    "            target_len = target.shape[0]\n",
    "        \n",
    "\n",
    "        batch_size = source.shape[1]\n",
    "        \n",
    "        target_vocab_size = self.output_dim\n",
    "        \n",
    "        outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(self.device)\n",
    "        \n",
    "        encoder_hidden = self.encoder(source, source_len)\n",
    "        \n",
    "        # mask = [batch_size, src len]\n",
    "        # without sos token at the beginning and eos token at the end\n",
    "        \n",
    "        #x = target[0,:]\n",
    "        decoder_input = torch.ones(batch_size).long().to(self.device)\n",
    "        decoder_hidden = encoder_hidden\n",
    "        encoder_outputs = None\n",
    "\n",
    "        for t in range(target_len):\n",
    "            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            outputs[t] = decoder_output\n",
    "            \n",
    "            #(N, english_vocab_size)\n",
    "            #best_guess = output.argmax(1)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "            \n",
    "            decoder_input = target[t] if random.random() < teacher_force_ratio and target is not None else decoder_input\n",
    "\n",
    "        return outputs\n",
    "    \n",
    "    def loss(self, logits, target):\n",
    "        return self._loss(logits, target)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.AdamW(self.parameters(), lr=self.learning_rate)\n",
    "        \n",
    "        lr_scheduler = {\n",
    "            'scheduler': optim.lr_scheduler.OneCycleLR(\n",
    "                optimizer,\n",
    "                max_lr = self.learning_rate,\n",
    "                steps_per_epoch = 3379,\n",
    "                epochs=self.max_epochs,\n",
    "                anneal_strategy='linear',\n",
    "                final_div_factor=1000,\n",
    "                pct_start = 0.01\n",
    "            ),\n",
    "            \"name\": \"learning_rate\",\n",
    "            \"interval\":\"step\",\n",
    "            \"frequency\": 1\n",
    "        }\n",
    "        \n",
    "        return [optimizer],[lr_scheduler]\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        src_seq, trg_seq, src_lengths = batch['src'],batch['trg'], batch['src_len']\n",
    "  \n",
    "        output = self.forward(src_seq, src_lengths,trg_seq)\n",
    "        \n",
    "        # do not know if this is a problem, loss will be computed with sos token\n",
    "        # without sos token at the beginning and eos token at the end\n",
    "\n",
    "        output = output.view(-1, self.output_dim)\n",
    "  \n",
    "        \n",
    "        trg_seq = trg_seq.transpose(0, 1)\n",
    "        trg = trg_seq.reshape(-1)\n",
    "        \n",
    "        loss = self.loss(output, trg)\n",
    "        \n",
    "        self.log('train_loss',loss.item(), \n",
    "                on_step = True,\n",
    "                on_epoch=True,\n",
    "                prog_bar = True,\n",
    "                logger=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch,batch_idx):\n",
    "        \"\"\" validation is in eval model so we do not have to use placeholder input sensors\"\"\"\n",
    "        src_seq, trg_seq, src_lengths = batch['src'],batch['trg'], batch['src_len']\n",
    "        \n",
    "        outputs = self.forward(src_seq, src_lengths, trg_seq, 0)\n",
    "        \n",
    "        logits = outputs[1:].view(-1, self.output_dim)\n",
    "        \n",
    "        trg = trg_seq[1:].reshape(-1)\n",
    "        \n",
    "        loss = self.loss(logits, trg)\n",
    "        \n",
    "        pred_seq = outputs[1:].argmax(2)   # seq_len*batch_size*vocab_size -> seq_len * batch_size\n",
    "        \n",
    "        # change layout: sesq_len * batch_size -> batch_size * seq_len\n",
    "        pred_seq = pred_seq.T\n",
    "        \n",
    "        # change layout: seq_len * batch_size -> batch_size * seq_len\n",
    "        trg_batch = trg_seq[1:].T\n",
    "        \n",
    "        # compare list of predicted ids for all sequences in a batch to targets\n",
    "        acc = plfunc.accuracy(pred_seq.reshape(-1), trg_batch.reshape(-1))\n",
    "        \n",
    "        # need to cast to list of predicted sequences ( as list of token ids ) [ seq_tok1, seqtok2]\n",
    "        predicted_ids - pred_seq.tolist()\n",
    "        \n",
    "        # need to add additional dim to each target reference sequence in order to\n",
    "        # conver to format needed by blue_score_func\n",
    "        # [seq1=[[reference1],[reference2]], seq2=[reference1]]\n",
    "        target_ids = torch.unsqueeze(trg_batch, 1).tolist()\n",
    "        \n",
    "        bleu_score - plfunc.nlp.bleu_score(predicted_ids, target_ids, n_gram=3).to(self.device)\n",
    "        \n",
    "        self.log(\n",
    "        'val_loss',\n",
    "        loss,\n",
    "        on_step=False,\n",
    "        on_epoch=True,\n",
    "        prog_bar=True,\n",
    "        logger=True,\n",
    "        sync_dist=True)\n",
    "        \n",
    "        self.log(\n",
    "            \"val_acc\",\n",
    "            acc,\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "            logger=True,\n",
    "            sync_dist=True\n",
    "        )\n",
    "        \n",
    "        self.log(\n",
    "            \"val_bleu_idx\",\n",
    "            bleu_score,\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "            logger=True,\n",
    "            sync_dist=True\n",
    "        )\n",
    "        \n",
    "        return loss, acc, bleu_score\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "variable-stanley",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_dataset.ipynb.\n",
      "Converted 01_seq2seq.ipynb.\n",
      "Converted index.ipynb.\n",
      "Converted tensorboard.ipynb.\n"
     ]
    }
   ],
   "source": [
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attended-poison",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
